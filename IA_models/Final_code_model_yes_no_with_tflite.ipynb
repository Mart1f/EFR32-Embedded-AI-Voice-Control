{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9k7omWZYpBA",
        "outputId": "fc4da6a9-6927-4ec6-c051-77aca2694fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip -q install tensorflow soundfile scipy scikit-learn\n",
        "\n",
        "import os, glob, zipfile, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import soundfile as sf\n",
        "import scipy.signal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "\n",
        "print(\"TF:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()  # sube student_yes_no_samples.zip\n",
        "zip_name = [k for k in uploaded.keys() if k.endswith(\".zip\")][0]\n",
        "\n",
        "os.makedirs(\"student_samples\", exist_ok=True)\n",
        "with zipfile.ZipFile(zip_name, \"r\") as z:\n",
        "    z.extractall(\"student_samples\")\n",
        "\n",
        "print(\"Student YES:\", len(glob.glob(\"student_samples/student_data/yes/*.wav\")))\n",
        "print(\"Student  NO:\", len(glob.glob(\"student_samples/student_data/no/*.wav\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "JRpcOK28YyjD",
        "outputId": "8978ee52-2414-490e-be7a-d32b509fce7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a5582c1f-16a4-4000-aab6-a52268f6f26a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a5582c1f-16a4-4000-aab6-a52268f6f26a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving student_yes_no_samples.zip to student_yes_no_samples.zip\n",
            "Student YES: 10\n",
            "Student  NO: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_URL = \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
        "!wget -q $DATASET_URL -O speech_commands.tar.gz\n",
        "!tar -xf speech_commands.tar.gz\n",
        "print(\"Speech Commands extracted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiPseM_fY46P",
        "outputId": "4f09fa99-602a-450f-e0c3-04be37b7de58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech Commands extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Frontend params: deben calzar con tu sl_ml_audio_feature_generation_config.h ===\n",
        "SR = 16000\n",
        "SAMPLE_LEN_MS = 1000\n",
        "WINDOW_MS = 25\n",
        "STEP_MS = 10\n",
        "FFT_LEN = 512\n",
        "N_CHANNELS = 40\n",
        "LOWER_HZ = 0.0\n",
        "UPPER_HZ = 8000.0  # <= SR/2\n",
        "LOG_SCALE = True\n",
        "\n",
        "def load_audio_1s(path):\n",
        "    x, sr = sf.read(path)\n",
        "    if x.ndim > 1:\n",
        "        x = x[:,0]\n",
        "    if sr != SR:\n",
        "        x = scipy.signal.resample_poly(x, SR, sr)\n",
        "    n = int(SR * (SAMPLE_LEN_MS/1000.0))\n",
        "    if len(x) < n:\n",
        "        x = np.pad(x, (0, n-len(x)))\n",
        "    else:\n",
        "        x = x[:n]\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "def hz_to_mel(hz): return 2595.0 * np.log10(1.0 + hz/700.0)\n",
        "def mel_to_hz(m):  return 700.0 * (10**(m/2595.0) - 1.0)\n",
        "\n",
        "def mel_filterbank(sr, nfft, n_mels, fmin, fmax):\n",
        "    # nfft -> bins = nfft/2+1\n",
        "    mels = np.linspace(hz_to_mel(fmin), hz_to_mel(fmax), n_mels + 2)\n",
        "    hz = mel_to_hz(mels)\n",
        "    bins = np.floor((nfft + 1) * hz / sr).astype(int)\n",
        "\n",
        "    fb = np.zeros((n_mels, nfft//2 + 1), dtype=np.float32)\n",
        "    for m in range(1, n_mels+1):\n",
        "        f0, f1, f2 = bins[m-1], bins[m], bins[m+1]\n",
        "        if f1 == f0: f1 += 1\n",
        "        if f2 == f1: f2 += 1\n",
        "        for k in range(f0, f1):\n",
        "            fb[m-1, k] = (k - f0) / (f1 - f0)\n",
        "        for k in range(f1, f2):\n",
        "            fb[m-1, k] = (f2 - k) / (f2 - f1)\n",
        "    return fb\n",
        "\n",
        "FBANK = mel_filterbank(SR, FFT_LEN, N_CHANNELS, LOWER_HZ, UPPER_HZ)\n",
        "\n",
        "def frontend_features(x):\n",
        "    win = int(SR * WINDOW_MS/1000.0)   # 240\n",
        "    step = int(SR * STEP_MS/1000.0)    # 160\n",
        "    # framing\n",
        "    frames = np.lib.stride_tricks.sliding_window_view(x, win)[::step].copy()\n",
        "    frames *= np.hamming(win).astype(np.float32)\n",
        "\n",
        "    # power spectrum\n",
        "    spec = np.fft.rfft(frames, n=FFT_LEN)\n",
        "    mag = np.abs(spec).astype(np.float32)\n",
        "    pow_spec = (mag ** 2) / FFT_LEN\n",
        "\n",
        "    # mel filterbank energies\n",
        "    mel_e = np.dot(pow_spec, FBANK.T)\n",
        "    mel_e = np.maximum(mel_e, 1e-10)\n",
        "\n",
        "    if LOG_SCALE:\n",
        "        mel_e = np.log(mel_e)\n",
        "\n",
        "    feat = mel_e.flatten().astype(np.float32)  # 49*32 = 1568\n",
        "    return feat\n",
        "\n",
        "# quick sanity: length must be 1568\n",
        "_test = frontend_features(np.zeros(int(SR*1.0), dtype=np.float32))\n",
        "print(\"Feature length:\", _test.shape[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RySfGx4xY-io",
        "outputId": "0b70a8b5-2e76-40d4-f8f6-7ae6bb4e9c55"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature length: 3920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"yes\", \"no\", \"unknown\", \"background\"]\n",
        "lab2i = {l:i for i,l in enumerate(labels)}\n",
        "\n",
        "student_yes = glob.glob(\"student_samples/student_data/yes/*.wav\")\n",
        "student_no  = glob.glob(\"student_samples/student_data/no/*.wav\")\n",
        "\n",
        "google_yes = glob.glob(\"yes/*.wav\")\n",
        "google_no  = glob.glob(\"no/*.wav\")\n",
        "\n",
        "# unknown: otras carpetas\n",
        "exclude = set([\"yes\",\"no\",\"_background_noise_\"])\n",
        "unknown_words = [\"up\",\"down\",\"left\",\"right\",\"stop\",\"go\",\"one\",\"two\",\"three\",\"four\"]\n",
        "unknown_pool = []\n",
        "for w in unknown_words:\n",
        "    unknown_pool += glob.glob(os.path.join(w, \"*.wav\"))\n",
        "random.shuffle(unknown_pool)\n",
        "\n",
        "# background noise wavs\n",
        "noise_files = glob.glob(\"_background_noise_/*.wav\")\n",
        "\n",
        "def sample_bg_chunks(noise_paths, n):\n",
        "    out = []\n",
        "    n_samp = int(SR * 1.0)\n",
        "    for _ in range(n):\n",
        "        p = random.choice(noise_paths)\n",
        "        a, sr = sf.read(p)\n",
        "        if a.ndim > 1: a = a[:,0]\n",
        "        if sr != SR:\n",
        "            a = scipy.signal.resample_poly(a, SR, sr)\n",
        "        if len(a) < n_samp:\n",
        "            a = np.pad(a, (0, n_samp-len(a)))\n",
        "            start = 0\n",
        "        else:\n",
        "            start = random.randint(0, len(a)-n_samp)\n",
        "        out.append(a[start:start+n_samp].astype(np.float32))\n",
        "    return out\n",
        "\n",
        "# balance\n",
        "MAX_GOOGLE = 1200\n",
        "google_yes = google_yes[:MAX_GOOGLE]\n",
        "google_no  = google_no[:MAX_GOOGLE]\n",
        "unknown_files = unknown_pool[:MAX_GOOGLE]\n",
        "bg_audios = sample_bg_chunks(noise_files, MAX_GOOGLE)\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "def add_files(files_list, lab):\n",
        "    for f in files_list:\n",
        "        a = load_audio_1s(f)\n",
        "        X.append(frontend_features(a))\n",
        "        y.append(lab2i[lab])\n",
        "\n",
        "add_files(student_yes, \"yes\")\n",
        "add_files(student_no, \"no\")\n",
        "add_files(google_yes, \"yes\")\n",
        "add_files(google_no, \"no\")\n",
        "add_files(unknown_files, \"unknown\")\n",
        "\n",
        "for a in bg_audios:\n",
        "    X.append(frontend_features(a))\n",
        "    y.append(lab2i[\"background\"])\n",
        "\n",
        "X = np.array(X, dtype=np.float32)\n",
        "y = np.array(y, dtype=np.int32)\n",
        "\n",
        "print(\"X:\", X.shape, \"y:\", y.shape)\n",
        "print(\"Counts:\", {l:int(np.sum(y==lab2i[l])) for l in labels})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRPffDnqZRiu",
        "outputId": "564409ad-59d7-47cd-e77e-ff61342350c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: (4820, 3920) y: (4820,)\n",
            "Counts: {'yes': 1210, 'no': 1210, 'unknown': 1200, 'background': 1200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input(shape=(train_x.shape[1],)),\n",
        "  tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(4, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_x, train_y, epochs=15, batch_size=32, validation_data=(test_x, test_y))\n",
        "print(\"Eval:\", model.evaluate(test_x, test_y, verbose=0))\n",
        "\n",
        "def rep_gen():\n",
        "    for i in range(min(300, len(train_x))):\n",
        "        yield [train_x[i:i+1].astype(np.float32)]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = rep_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_int8 = converter.convert()\n",
        "\n",
        "out_name = \"keyword_spotting_on_off.tflite\"\n",
        "with open(out_name, \"wb\") as f:\n",
        "    f.write(tflite_int8)\n",
        "\n",
        "# sanity check\n",
        "interp = tf.lite.Interpreter(model_path=out_name)\n",
        "interp.allocate_tensors()\n",
        "out = interp.get_output_details()[0]\n",
        "print(\"Output dtype/shape:\", out[\"dtype\"], out[\"shape\"])\n",
        "\n",
        "files.download(out_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "Vt4rTCT8Zj4G",
        "outputId": "3dc0effa-c8e1-4834-d98d-4f14d53f2ad0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.3641 - loss: 14.8983 - val_accuracy: 0.6162 - val_loss: 1.2828\n",
            "Epoch 2/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5501 - loss: 1.7334 - val_accuracy: 0.6556 - val_loss: 0.8885\n",
            "Epoch 3/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5730 - loss: 1.5528 - val_accuracy: 0.6722 - val_loss: 2.6747\n",
            "Epoch 4/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6396 - loss: 1.3021 - val_accuracy: 0.7344 - val_loss: 0.6682\n",
            "Epoch 5/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6717 - loss: 1.1337 - val_accuracy: 0.6753 - val_loss: 1.2179\n",
            "Epoch 6/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6851 - loss: 0.8751 - val_accuracy: 0.7448 - val_loss: 0.6435\n",
            "Epoch 7/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7113 - loss: 0.7745 - val_accuracy: 0.7324 - val_loss: 0.7203\n",
            "Epoch 8/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7513 - loss: 0.6756 - val_accuracy: 0.5124 - val_loss: 1.6161\n",
            "Epoch 9/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7110 - loss: 0.8003 - val_accuracy: 0.7573 - val_loss: 0.6100\n",
            "Epoch 10/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7492 - loss: 0.6283 - val_accuracy: 0.6535 - val_loss: 0.9457\n",
            "Epoch 11/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7008 - loss: 0.7751 - val_accuracy: 0.6898 - val_loss: 0.7895\n",
            "Epoch 12/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7055 - loss: 0.7643 - val_accuracy: 0.6898 - val_loss: 0.7428\n",
            "Epoch 13/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7199 - loss: 0.7173 - val_accuracy: 0.7168 - val_loss: 0.7868\n",
            "Epoch 14/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7219 - loss: 0.6555 - val_accuracy: 0.7085 - val_loss: 0.6766\n",
            "Epoch 15/15\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7467 - loss: 0.6121 - val_accuracy: 0.7199 - val_loss: 0.6991\n",
            "Eval: [0.699112057685852, 0.7199169993400574]\n",
            "Saved artifact at '/tmp/tmp8foesxg0'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 3920), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135988343289232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135988343290384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135988343289040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135988343287696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135988343290960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135988343287888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output dtype/shape: <class 'numpy.int8'> [1 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ed51d650-f8c2-4bda-ab74-3cc8ebcbc04d\", \"keyword_spotting_on_off.tflite\", 518160)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5odqDKiaXc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}